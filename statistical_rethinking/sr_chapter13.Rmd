---
title: "Soc723 - SR Chapter 13"
author: "Turgut Keskint√ºrk"
date: "`r Sys.Date()`"
output:
  html_document:
    theme: united
    toc: true
    number_sections: false
    code_folding: "hide"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(warning = FALSE, message = FALSE)
knitr::opts_chunk$set(fig.align = "center")

# load the relevant packages
pacman::p_load(
  tidyverse,
  purrr,
  rethinking,
  rstan,
  tidybayes,
  tidybayes.rethinking,
  patchwork,
  hrbrthemes,
  knitr
)
theme_set(theme_ipsum_rc()) # theme set for the plots

```

Hello there! Let's start with our exercises for McElreath's *Statistical Rethinking*, Chapter 13.

# Easy Problems

## 13E1

The shrinkage means that the model will not get too excited by the data; so, the answer is **(a)**, as the variation it allows is more limited.

## 13E2

I assume a model with varying intercepts but no varying slopes (just want to say *fixed slopes* here, but I get it, McElreath):

$$
\begin{aligned}
y_i &\sim \text{Binomial}(1, p_i)
\\
\text{logit}(p_i) &= \alpha_{GROUP[i]} + \beta x_i
\\
\alpha_{j} &\sim \text{Normal}(\theta, \sigma) \ & \text{Adaptive prior}
\\
\theta &\sim \text{Normal}(0, 1.5)  \ & \text{Prior for the group guys}
\\
\sigma &\sim \text{Exponential}(1)  \ & \text{Standard deviation for the group guys}
\\
\beta &\sim \text{Normal}(0, 0.5) \ & \text{The boring fixed effects prior}
\end{aligned}
$$

## 13E3

I still assume a model with varying intercepts but no varying slopes:

$$
\begin{aligned}
y_i &\sim \text{Normal}(\mu_i, \sigma)
\\
\mu_i &= \alpha_{GROUP[i]} + \beta x_i
\\
\alpha_{j} &\sim \text{Normal}(\theta, \tau) \ & \text{Adaptive prior}
\\
\theta &\sim \text{Normal}(0, 5)  \ & \text{Prior for the group guys}
\\
\tau &\sim \text{Exponential} (1) \ & \text{Standard deviation for the group guys}
\\
\sigma &\sim \text{Exponential}(1)  \ & \text{This is for the likelihood this time!}
\\
\beta &\sim \text{Normal}(0, 1)
\end{aligned}
$$

## 13E4

Alright, time for Poisson:

$$
\begin{aligned}
y_i &\sim \text{Poisson}(\lambda_i)
\\
\text{log}(\lambda_i) &= \alpha_{GROUP[i]}
\\
\alpha_{j} &\sim \text{Normal}(\theta, \sigma) \ & \text{Adaptive prior}
\\
\theta &\sim \text{Normal}(0, 1)  \ & \text{Prior for the group guys}
\\
\sigma &\sim \text{Exponential}(1)  \ & \text{Standard deviation for the group guys}
\end{aligned}
$$

## 13E5

Nice, this time we will have two terms!

$$
\begin{aligned}
y_i &\sim \text{Poisson}(\lambda_i)
\\
\text{log}(\lambda_i) &= \alpha_{GROUP1[i]} + \beta_{GROUP2[i]}
\\
\alpha_{j} &\sim \text{Normal}(\theta, \sigma_1) \ & \text{Adaptive prior for Group 1}
\\
\beta_{j} &\sim \text{Normal}(0, \sigma_2) \ & \text{Adaptive prior for Group 2}
\\
\theta &\sim \text{Normal}(0, 1)  \ & \text{Prior for Group 1}
\\
\sigma_1 &\sim \text{Exponential}(1)  \ & \text{Standard deviation for Group 1}
\\
\sigma_2 &\sim \text{Exponential}(1)  \ & \text{Standard deviation for Group 2}
\end{aligned}
$$

# Medium Problems

OK, we are going to use the `reedfrogs` data for the upcoming four questions. Let's load it and prepare it for analyses.

```{r}

# load
data(reedfrogs)

# add an index & generate integers
d <- reedfrogs |> 
  mutate(tank = 1:nrow(reedfrogs),
         size = ifelse(size == "small", 1, 2),
         pred = ifelse(pred == "no", 1, 2),
         term = case_when( # an interaction variable for ease of use later
           size == 1 & pred == 1 ~ 1,
           size == 1 & pred == 2 ~ 2,
           size == 2 & pred == 1 ~ 3,
           size == 2 & pred == 2 ~ 4))
  
# generate lists of data
d1 <- list(tank = d$tank, S = d$surv, N = d$density) # this is for using the data without the treatment variables
d2 <- list(tank = d$tank, S = d$surv, N = d$density, size = d$size, pred = d$pred) # all variables
d3 <- list(tank = d$tank, S = d$surv, N = d$density, size = d$size) # with size
d4 <- list(tank = d$tank, S = d$surv, N = d$density, pred = d$pred) # with pred
d5 <- list(tank = d$tank, S = d$surv, N = d$density, size = d$size, pred = d$pred, term = d$term) # interaction data 

```

Let's also fit the model used in the textbook:

```{r, message = FALSE, warning = FALSE, results = "hide"}

m1 <- ulam(
  alist(
    S ~ dbinom(N, p),
    logit(p) <- a[tank],
    a[tank] ~ dnorm(a_bar, sigma),
    a_bar ~ dnorm(0, 1.5),
    sigma ~ dexp(1)
  ), data = d1, chains = 4, cores = 4, log_lik = T
)

```

## 13M1

OK, we are going to fit several models for comparison: (a) either treatment alone, (b) both treatments, and (c) their interactions. Let's do it:

```{r, message = FALSE, warning = FALSE, results = "hide"}

# model with size
m2 <- ulam(
  alist(
    S ~ dbinom(N, p),
    logit(p) <- a[tank] + b[size],
    a[tank] ~ dnorm(a_bar, sigma),
    a_bar ~ dnorm(0, 1.5),
    sigma ~ dexp(1),
    b[size] ~ dnorm(0, 1)
  ), data = d3, chains = 4, cores = 4, log_lik = T
)

# model with pred
m3 <- ulam(
  alist(
    S ~ dbinom(N, p),
    logit(p) <- a[tank] + b[pred],
    a[tank] ~ dnorm(a_bar, sigma),
    a_bar ~ dnorm(0, 1.5),
    sigma ~ dexp(1),
    b[pred] ~ dnorm(0, 1)
  ), data = d4, chains = 4, cores = 4, log_lik = T
)

# model with both treatments
m4 <- ulam(
  alist(
    S ~ dbinom(N, p),
    logit(p) <- a[tank] + b1[size] + b2[pred],
    a[tank] ~ dnorm(a_bar, sigma),
    a_bar ~ dnorm(0, 1.5),
    sigma ~ dexp(1),
    b1[size] ~ dnorm(0, 1),
    b2[pred] ~ dnorm(0, 1)
  ), data = d2, chains = 4, cores = 4, log_lik = T
)

# model with both treatments and their interactions
m5 <- ulam(
  alist(
    S ~ dbinom(N, p),
    logit(p) <- a[tank] + b1[size] + b2[pred] + b3[term],
    a[tank] ~ dnorm(a_bar, sigma),
    a_bar ~ dnorm(0, 1.5),
    sigma ~ dexp(1),
    b1[size] ~ dnorm(0, 1),
    b2[pred] ~ dnorm(0, 1),
    b3[term] ~ dnorm(0, 1)
  ), data = d5, chains = 4, cores = 4, log_lik = T
)

```

We fitted the models. McElreath wants us to think about the *inferred variation across tanks*, which is basically the $\sigma$ parameter. Let me pull the posterior distributions of these parameters from each model and visualize the changes in $\sigma$:

```{r, fig.width = 8, fig.height = 4, dpi = 300}

bind_rows(
  precis(m1, depth = 2, pars = "sigma") |> t() |> as_tibble() |> janitor::clean_names(),
  precis(m2, depth = 2, pars = "sigma") |> t() |> as_tibble() |> janitor::clean_names(),
  precis(m3, depth = 2, pars = "sigma") |> t() |> as_tibble() |> janitor::clean_names(),
  precis(m4, depth = 2, pars = "sigma") |> t() |> as_tibble() |> janitor::clean_names(),
  precis(m5, depth = 2, pars = "sigma") |> t() |> as_tibble() |> janitor::clean_names()
) |> mutate(
  models = c("Baseline Model", "Model with Size", "Model with Pred", "Model with Both", "Interaction Model")
) |> 
  ggplot(aes(x = reorder(models, mean), y = mean)) +
  geom_point() +
  geom_linerange(aes(ymin = x5_5_percent, ymax = x94_5_percent)) +
  labs(title = "Posterior Distribution of Sigma Across Models", 
       x = "Models", y = "Sigma") +
  coord_flip()

```

It seems that not `size`, but `pred` treatment explains an important portion of the variation across tanks.

## 13M2

Let's compare these models to have a sense of model fit:

```{r}

rethinking::compare(m1, m2, m3, m4, m5) |> 
  # prettify the results
  as_tibble() |> 
  mutate(models = c(
    "Model with Pred", "Model with Interactions", "Model with Size", "Model with Both", "Baseline Model")) |> 
  relocate(models) |> 
  kable(digits = 2, align = "c")

```

As we can see, the models including `pred` tends to perform better, though the differences do not really amount to anything. We saw before that the `pred` is particularly good in terms of reducing $\sigma$, but well, the model that ascribes variation to tanks performs as better.

To be honest, out-of-sample prediction is not everything!

## 13M3

Now, we are going to fit the same varying intercept model with a Cauchy distribution. Here is the model:

```{r, message = FALSE, warning = FALSE, results = "hide"}

# multilevel model with cauchy distribution
set.seed(1123)
m6 <- ulam(
  alist(
    S ~ dbinom(N, p),
    logit(p) <- a[tank],
    a[tank] ~ dcauchy(a_bar, sigma),
    a_bar ~ dnorm(0, 1.5),
    sigma ~ dexp(1)
  ), data = d1, chains = 4, cores = 4, log_lik = T
)

```

Well, no divergent transitions. Looking at the trankplots, though, I saw that there are some wild behaviors of certain parameters. Let's look at the `n_eff` values for all parameters to spot the problems:

```{r, fig.width = 7.5, fig.height = 10, dpi = 300}

tibble(
  param = rownames(precis(m6, depth = 2)),
  n_eff = precis(m6, depth = 2) |> as_tibble() |> pull(n_eff)) |> 
  filter(param != "a_bar" & param != "sigma") |> 
  ggplot(aes(x = reorder(param, n_eff), n_eff)) +
  geom_point() +
  coord_flip() +
  labs(title = "Effective Sample Size Across Parameters", x = "Parameters", y = "Effective Sample Size")

```

Yeah, some of the `n_eff` values are horrible. Let's follow McElreath and fit the model once more, this time changing acceptance rate:

```{r, message = FALSE, warning = FALSE, results = "hide"}

# multilevel model with cauchy distribution, revised
set.seed(1123)
m7 <- ulam(m6, data = d1, chains = 4, cores = 4, log_lik = T, control = list(adapt_delta = 0.99))

```

This time a lot of warnings (algorithmic issues), though no divergent transitions again. Let's see the `n_eff` values:

```{r, fig.width = 7.5, fig.height = 10, dpi = 300}

tibble(
  param = rownames(precis(m7, depth = 2)),
  n_eff = precis(m7, depth = 2) |> as_tibble() |> pull(n_eff)) |> 
  filter(param != "a_bar" & param != "sigma") |> 
  ggplot(aes(x = reorder(param, n_eff), n_eff)) +
  geom_point() +
  coord_flip() +
  labs(title = "Effective Sample Size Across Parameters", x = "Parameters", y = "Effective Sample Size")

```

I guess I am going to need to reparametrize. Let's do it:

```{r, message = FALSE, warning = FALSE, results = "hide"}

# multilevel model with cauchy distribution, second revision
set.seed(1123)
m8 <- ulam(
  alist(
    S ~ dbinom(N, p),
    logit(p) <- a_bar + z[tank]*sigma,
    z[tank] ~ dcauchy(0, 1),
    a_bar ~ dnorm(0, 1.5),
    sigma ~ dexp(1),
    gq> vector[tank]:a <<- a_bar + z*sigma
  ), data = d1, chains = 4, cores = 4, log_lik = T
)

```

OK, I got no warnings, neither divergent transitions nor algorithmic problems. Let's inspect the `n_eff` values once more:

```{r, fig.width = 7.5, fig.height = 7.5, dpi = 300}

tibble(
  param = rownames(precis(m8, depth = 2)),
  n_eff = precis(m8, depth = 2) |> as_tibble() |> pull(n_eff)
) |>
  filter(str_detect(param, "a")) |>
  filter(param != "a_bar" & param != "sigma") |>
  left_join(
    tibble(
      param = rownames(precis(m7, depth = 2)),
      n_eff = precis(m7, depth = 2) |> as_tibble() |> pull(n_eff)
    ) |>
      filter(param != "a_bar" & param != "sigma"),
    by = "param"
  ) |> 
  ggplot(aes(x = n_eff.x, y = n_eff.y)) +
  geom_point() +
  geom_abline(intercept = 0, slope = 1) + 
  labs(title = "Effective Sample Size Across Parameters", x = "Parametrized Model", y = "Non-Parametrized Model")

```

It seems that this is somewhat better, though there is a clump of parameters close to 0. Let's compare the posterior means of these models:

```{r, fig.width = 7.5, fig.height = 7.5, dpi = 300}

tibble(
  param = rownames(precis(m8, depth = 2)),
  n_eff = precis(m8, depth = 2) |> as_tibble() |> pull(mean)
) |>
  filter(str_detect(param, "a")) |>
  filter(param != "a_bar" & param != "sigma") |>
  left_join(
    tibble(
      param = rownames(precis(m1, depth = 2)),
      n_eff = precis(m1, depth = 2) |> as_tibble() |> pull(mean)
    ) |>
      filter(param != "a_bar" & param != "sigma"),
    by = "param"
  ) |> 
  ggplot(aes(x = n_eff.x, y = n_eff.y)) +
  geom_point() +
  geom_abline(intercept = 0, slope = 1) + 
  labs(title = "Posterior Means", x = "Cauchy Model", y = "Gaussian Model") +
  xlim(-3, 12) + ylim(-3, 12)

```

Interesting! We see that the Cauchy distribution makes some posterior predictions much more of an "outlier," owing to its thick tails!

## 13M4

Another one! This time with Student-t. Let's do it:

```{r, message = FALSE, warning = FALSE, results = "hide"}

# multilevel model with student-t distribution
set.seed(1123)
m9 <- ulam(
  alist(
    S ~ dbinom(N, p),
    logit(p) <- a[tank],
    a[tank] ~ dstudent(2, a_bar, sigma),
    a_bar ~ dnorm(0, 1.5),
    sigma ~ dexp(1)
  ), data = d1, chains = 4, cores = 4, log_lik = T
)

```

Awesome. Here are the posterior means among Gaussian, Cauchy and Student-t distributions:

```{r, fig.width = 15, fig.height = 7.5, dpi = 300}

p1 <- tibble(
  param = rownames(precis(m9, depth = 2)),
  n_eff = precis(m9, depth = 2) |> as_tibble() |> pull(mean)
) |>
  filter(str_detect(param, "a")) |>
  filter(param != "a_bar" & param != "sigma") |>
  left_join(
    tibble(
      param = rownames(precis(m1, depth = 2)),
      n_eff = precis(m1, depth = 2) |> as_tibble() |> pull(mean)
    ) |>
      filter(param != "a_bar" & param != "sigma"),
    by = "param"
  ) |> 
  ggplot(aes(x = n_eff.x, y = n_eff.y)) +
  geom_point() +
  geom_abline(intercept = 0, slope = 1) + 
  labs(title = "Posterior Means", x = "Student-t Model", y = "Gaussian Model") +
  xlim(-3, 12) + ylim(-3, 12)

p2 <- tibble(
  param = rownames(precis(m9, depth = 2)),
  n_eff = precis(m9, depth = 2) |> as_tibble() |> pull(mean)
) |>
  filter(str_detect(param, "a")) |>
  filter(param != "a_bar" & param != "sigma") |>
  left_join(
    tibble(
      param = rownames(precis(m8, depth = 2)),
      n_eff = precis(m8, depth = 2) |> as_tibble() |> pull(mean)
    ) |>
      filter(param != "a_bar" & param != "sigma"),
    by = "param"
  ) |> 
  ggplot(aes(x = n_eff.x, y = n_eff.y)) +
  geom_point() +
  geom_abline(intercept = 0, slope = 1) + 
  labs(title = "Posterior Means", x = "Student-t Model", y = "Cauchy Model") +
  xlim(-3, 12) + ylim(-3, 12)

p1 + p2

```

Really interesting! It seems that, compared to the Gaussian distribution, both Cauchy and Student-t distributions allow more "outlier" values. Once we compare the Student-t with the Cauchy model, though, we see that Student-t is much more modest!

## 13M5

Hello chimpanzees. Let's load the data and fit the model from the book:

```{r, message = FALSE, warning = FALSE, results = "hide"}

# data issues
data(chimpanzees)
d <- chimpanzees |> 
  mutate(treatment = 1 + prosoc_left + 2*condition)
d <- list(
  pulled_left = d$pulled_left,
  actor = d$actor,
  block_id = d$block,
  treatment = as.integer(d$treatment)
)

# model
m1 <-ulam(
  alist(
    pulled_left ~ dbinom(1, p),
    logit(p) <- a[actor] + g[block_id] + b[treatment],
    b[treatment] ~ dnorm(0, 0.5),
    ## adaptive stuff
    a[actor] ~ dnorm(a_bar, sigma_a),
    g[block_id] ~ dnorm(0, sigma_g),
    ## hyper-priors
    a_bar ~ dnorm(0, 1.5),
    sigma_a ~ dexp(1),
    sigma_g ~ dexp(1)),
  data = d, chains = 4, cores = 4, log_lik = TRUE)

```

Alright. I am going to modify this model so that `block` will contain a new parameter $\hat{\gamma}$. This gets at my collinearity issue, I suppose.

```{r, message = FALSE, warning = FALSE, results = "hide"}

m2 <-ulam(
  alist(
    pulled_left ~ dbinom(1, p),
    logit(p) <- a[actor] + g[block_id] + b[treatment],
    b[treatment] ~ dnorm(0, 0.5),
    ## adaptive stuff
    a[actor] ~ dnorm(a_bar, sigma_a),
    g[block_id] ~ dnorm(gamma, sigma_g),
    ## hyper-priors
    a_bar ~ dnorm(0, 1.5),
    gamma ~ dnorm(0, 1.5),
    sigma_a ~ dexp(1),
    sigma_g ~ dexp(1)),
  data = d, chains = 4, cores = 4, log_lik = TRUE)

```

We have the models. Let's first compare the model performance:

```{r}

rethinking::compare(m1, m2) |> 
  # prettify the results
  as_tibble() |> 
  mutate(models = c(
    "Model without Gamma", "Model with Gamma")) |> 
  relocate(models) |> 
  kable(digits = 2, align = "c")

```

No difference. Let's compare the posterior distribution of our parameters, which I think will show some of the action:

```{r, fig.width = 10, fig.height = 7.5, dpi = 300}

tibble(
  param = coef(m1) |> 
    names() |> as_tibble() |> slice(1:20) |> pull(),
  m1 = rethinking::coef(m1) |> 
    as_tibble() |> slice(1:20) |> pull(),
  m2 = rethinking::coef(m2) |> 
    as_tibble() |> slice(1:20) |> pull()
) |> 
  ggplot() +
  geom_point(aes(x = reorder(param, desc(param)), y = m1), shape = 16, col = "red") +
  geom_point(aes(x = reorder(param, desc(param)), y = m2), shape = 17, col = "blue") +
  labs(title = "Posterior Distribution of Parameters Across Two Models", 
       x = "Models", y = "Value") + coord_flip()

```

There is not much difference for intercepts, though the $\sigma$ parameters changed strongly. Notice that the emphasis put on $\sigma_A$ and $\sigma_B$ is actually reversed: the inclusion of $\gamma$ parameter resulted in a reemphasis of the uncertainty around the parameters. **Overparametrization!**

## 13M6

Let's prepare a dataframe with one observation and fit four models according to McElreath's specifications below:

```{r, message = FALSE, warning = FALSE, results = "hide"}

d <- list(y = 0)

# model NN
m1 <- ulam(
  alist(
    y ~ dnorm(mu, 1), mu <- a, a ~ dnorm(10, 1) 
  ), data= d)

# model NT
m2 <- ulam(
  alist(
    y ~ dnorm(mu, 1), mu <- a, a ~ dstudent(2, 10, 1)
  ), data= d)

# model TN
m3 <- ulam(
  alist(
    y ~ dstudent(2, mu, 1), mu <- a, a ~ dnorm(10, 1)
  ), data= d)

# model TT
m4 <- ulam(
  alist(
    y ~ dstudent(2, mu, 1), mu <- a, a ~ dstudent(2, 10, 1)
  ), data= d)

```

Alright, let's get the posterior distributions and visualize the differences:

```{r, fig.width = 7,5, fig.height = 5, dpi = 300}

bind_cols(
  tidybayes::tidy_draws(m1) |> select(NN = a),
  tidybayes::tidy_draws(m2) |> select(NT = a),
  tidybayes::tidy_draws(m3) |> select(TN = a),
  tidybayes::tidy_draws(m4) |> select(TT = a)
) |>
  pivot_longer(
    cols = everything(),
    names_to = "Model",
    values_to = "Draws"
  ) |>
  ggplot(aes(x = Draws)) +
  geom_density(aes(fill = Model), alpha = 0.5) +
  scale_fill_manual(values = wesanderson::wes_palette("IsleofDogs1", n = 4)) +
  theme(legend.position = "top") +
  labs(title = "Posterior Draws", x = "Draws", y = "Density")

```

As we can see, the tails are much wider once the Student-t distribution comes into the play.

# Hard Questions

## 13H1

Alright, this seems interesting. Let's load the data.

```{r}

data(bangladesh)
d <- bangladesh |> 
  mutate(district_id = as.integer(as.factor(district)))
d <- list(d = d$district_id, c = d$use.contraception)

```

Aha, I hear some words from you, Mr. McElreath! I am going to fit a *fixed effects* model and a *multilevel model* below:

```{r, message = FALSE, warning = FALSE, results = "hide"}

# fixed effects model
m1 <- ulam(
  alist(
    c ~ dbinom(1, p),
    logit(p) <- a[d],
    a[d] ~ dnorm(0, 1)
  ), data = d, chains = 4, cores = 4, log_lik = T
)

# multilevel model
m2 <- ulam(
  alist(
    c ~ dbinom(1, p),
    logit(p) <- a[d],
    a[d] ~ dnorm(a_bar, sigma),
    a_bar ~ dnorm(0, 1),
    sigma ~ dexp(1)
  ), data = d, chains = 4, cores = 4, log_lik = T
)

```

Awesome! McElreath now wants us to plot the predicted proportions of women in each district. Here are the posterior distributions:

```{r, fig.width = 15, fig.height = 5, dpi = 300}

# predictions from fixed models
pred1 <- tibble(d = 1:60) |> 
  add_linpred_draws(m1, transform = T) |>
  janitor::clean_names() |> 
  group_by(d) |> 
  summarize(c1 = mean(linpred))

# predictions from multilevel model
pred2 <- tibble(d = 1:60) |> 
  add_linpred_draws(m2, transform = T) |>
  janitor::clean_names() |> 
  group_by(d) |> 
  summarize(c2 = mean(linpred))

# merge
pred <- inner_join(pred1, pred2, by = "d")

# plot
pred |>
  ggplot() +
  geom_point(aes(x = factor(d), y = c1),
             size = 2,
             shape = 14,
             col = "blue") +
  geom_point(aes(x = factor(d), y = c2),
             size = 2,
             shape = 18,
             col = "red") +
  geom_hline(aes(yintercept = mean(c1))) +
  theme_ipsum(grid = FALSE) +
  labs(title = "% Predictions Across Two Models", x = "Districts", y = "%")

```

As we can see, the multilevel model (red points) shrank the estimates toward the mean. This is what we expected and talked about in class.

## 13H2

OK, I am going to work with the `Trolley` data. Let's load and wrangle.

```{r}

data(Trolley)
d <- Trolley
d <- list(
  id = coerce_index(d$id),
  R = d$response,
  A = d$action,
  I = d$intention,
  C = d$contact
)

```

Two models - one with fixed effects and one with varying intercepts. I am going to treat the response variable as continuous for these models.

```{r, message = FALSE, warning = FALSE, results = "hide"}

m1 <- ulam(
  alist(
    R ~ dnorm(mu, sigma),
    mu <- a + b1*A + b2*I + b3*C,
    c(a, b1, b2, b3) ~ dnorm(0, 2.5),
    sigma ~ dexp(1)
  ), data = d, chains = 4, cores = 4, log_lik = T
)

m2 <- ulam(
  alist(
    R ~ dnorm(mu, sigma),
    mu <- a + b1*A + b2*I + b3*C + s[id],
    c(a, b1, b2, b3) ~ dnorm(0, 2.5),
    s[id] ~ dnorm(0, sigma),
    sigma ~ dexp(1),
    sigma ~ dexp(1)
  ), data = d, chains = 4, cores = 4, log_lik = T
)

```

**Comparison: Model Performance**. Let's use WAIC to compare these models.

```{r}

rethinking::compare(m1, m2) |> 
  # prettify the results
  as_tibble() |> 
  mutate(models = c(
    "Model with Varying Intercepts", "Model without Varying Intercepts")) |> 
  relocate(models) |> 
  kable(digits = 2, align = "c")

```

As expected. The model with varying intercepts has much more predictive capacity than the one without varying intercepts. Once we look at the output, we also see that the `sigma` in Model 1 has a strong magnitude, indicating that individual variation is powerful in this model.

## 13H3

Let's add the `story` to our list and specify a cross-classified multilevel model:

```{r, message = FALSE, warning = FALSE, results = "hide"}

# data
d <- Trolley
d <- list(
  id = coerce_index(d$id),
  R = d$response,
  A = d$action,
  I = d$intention,
  C = d$contact,
  S = d$story
)

# model
m3 <- ulam(
  alist(
    R ~ dnorm(mu, sigma1),
    mu <- a + b1*A + b2*I + b3*C + s1[id] + s2[S],
    c(a, b1, b2, b3) ~ dnorm(0, 2.5),
    s1[id] ~ dnorm(0, sigma2),
    s2[S] ~ dnorm(0, sigma3),
    sigma1 ~ dexp(1),
    sigma2 ~ dexp(1),
    sigma3 ~ dexp(1)
  ), data = d, chains = 4, cores = 4, log_lik = T
)

```

**Comparison: Model Performance**. Let's use WAIC to compare the models.

```{r}

rethinking::compare(m1, m2, m3) |> 
  # prettify the results
  as_tibble() |> 
  mutate(models = c(
    'Cross-Classified Model', "Model with Varying Intercepts", "Model without Varying Intercepts")) |> 
  relocate(models) |> 
  kable(digits = 2, align = "c")

```

Awesome! The  cross-classified model is much better, as different stories pick up a lot of variation from the data.

## 13H4

This is the same question as 13M1. Nice!

Thank you!
